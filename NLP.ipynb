{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "INFO - 2019-05-14 18:19:13,267 - Reading file...\n",
      "INFO - 2019-05-14 18:19:33,418 - Split reviews to 500 positive and 500 negative.\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# 1. Выделить из корпуса по 500 примеров на каждый класс, если классификация бинарная и по 200, если классов несколько.\n",
    "import logging\n",
    "import ast\n",
    "CORPUS_PATH = 'data/corpus.txt'\n",
    "\n",
    "log = logging.getLogger()\n",
    "\n",
    "POS_MARK = 1\n",
    "NEG_MARK = 0\n",
    "\n",
    "def setup_logger():\n",
    "    logging.basicConfig(format='%(levelname)s - %(asctime)s - %(message)s')\n",
    "    log.setLevel(logging.INFO)\n",
    "    \n",
    "def read_data(corpus_path):\n",
    "    with open(corpus_path, 'r') as f:\n",
    "        log.info(\"Reading file...\")\n",
    "\n",
    "        reviews = []\n",
    "        marks = []\n",
    "\n",
    "        lines = f.readlines()\n",
    "        lines = [x.strip() for x in lines]\n",
    "\n",
    "        for line in lines:\n",
    "            dictionary = ast.literal_eval(line)\n",
    "            reviews.append(dictionary['description'])\n",
    "\n",
    "            recom_author_mark = dictionary['recom_author_mark']\n",
    "            marks.append(parse_label(recom_author_mark))\n",
    "\n",
    "    return reviews, marks\n",
    "\n",
    "\n",
    "def parse_label(recom_author_mark):\n",
    "    if recom_author_mark == 'ДА':\n",
    "        return POS_MARK\n",
    "    elif recom_author_mark == '':\n",
    "        return NEG_MARK\n",
    "    else:\n",
    "        raise Exception(\"Unknown recom_author_mark\")\n",
    "\n",
    "\n",
    "def split_reviews(reviews, marks, n=500):\n",
    "    pos_reviews = []\n",
    "    neg_reviews = []\n",
    "\n",
    "    if len(reviews) < n or n == -1:\n",
    "        n = len(reviews)\n",
    "\n",
    "    log.info(f\"Split reviews to {n} positive and {n} negative.\")\n",
    "\n",
    "    for index, review in enumerate(reviews):\n",
    "        if marks[index] == POS_MARK and len(pos_reviews) < n:\n",
    "            pos_reviews.append(review)\n",
    "        elif marks[index] == NEG_MARK and len(neg_reviews) < n:\n",
    "            neg_reviews.append(review)\n",
    "\n",
    "        if len(pos_reviews) == n and len(neg_reviews) == n:\n",
    "            break\n",
    "\n",
    "    return pos_reviews, neg_reviews\n",
    "\n",
    "setup_logger()\n",
    "\n",
    "reviews, marks = read_data(CORPUS_PATH)\n",
    "pos_reviews, neg_reviews = split_reviews(reviews, marks)\n",
    "\n",
    "y_pos = [1] * len(pos_reviews)\n",
    "y_neg = [0] * len(neg_reviews)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# 2. Разбить примеры на обучающую и тестовую выборки в соотношении 80% к 20% соответственно.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(pos_reviews + neg_reviews, y_pos + y_neg, test_size=0.2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "INFO - 2019-05-14 18:31:30,393 - Precision: 0.857, recall: 0.824, f-measure: 0.84\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# 3. Реализовать классификатор на основе tf-idf и модели логистической регрессии, вывести метрики качества на тестовом множестве. Максимальное кол-во признаков: 50000, минимальная частота: 5.\n",
    "import string\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from pymystem3 import Mystem\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def fit_and_print_score(x_train, y_train, x_test, y_test):\n",
    "    model = LogisticRegression(solver=\"lbfgs\")\n",
    "    model.fit(x_train, y_train)\n",
    "    print_score_model(model, x_test, y_test)\n",
    "    \n",
    "def print_score_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    metrics = precision_recall_fscore_support(y_pred=y_pred, y_true=y_test, average='binary', pos_label=POS_MARK)\n",
    "    log.info(f'Precision: {round(metrics[0], 3)}, recall: {round(metrics[1], 3)}, f-measure: {round(metrics[2], 3)}')\n",
    "    \n",
    "def tokenize(document):\n",
    "    ignore = set(stopwords.words('russian'))\n",
    "    stem = Mystem()\n",
    "\n",
    "    tokens = stem.lemmatize(document)\n",
    "\n",
    "    tokens = [w.lower() for w in tokens if w not in ignore]\n",
    "    tokens = [w for w in tokens if w not in string.punctuation]\n",
    "    tokens = [w for w in tokens if w.isalpha()]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=50000, min_df=5, tokenizer=tokenize)\n",
    "X_train_vect = vectorizer.fit_transform(X_train)\n",
    "X_test_vect = vectorizer.transform(X_test)\n",
    "\n",
    "fit_and_print_score(X_train_vect.toarray(), y_train, X_test_vect.toarray(), y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "INFO - 2019-05-14 18:31:30,405 - Features importance:\n",
      "/home/maxim/anaconda3/envs/nlp/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": [
      "('мат', 'рд', 'бесплатно', 'измерение', 'медперсонал', 'врач', 'нагрубить', 'тихий', 'ребенок', 'особо', 'сказать', 'вежливый', 'туда', 'слезать', 'очень', 'вместе', 'ничто', 'линза', 'вывод', 'душ', 'плюс', 'право', 'рождество', 'предупреждать', 'обход', 'гуляля', 'уходить', 'именно', 'программа', 'внушать', 'шесть', 'недоплачивать', 'паспорт', 'действие', 'анализ', 'договор', 'спрашивать', 'считать', 'проходить', 'тысяча', 'орать', 'нужный', 'дело', 'приходить', 'плохой', 'ждать', 'платить', 'жаль', 'услышать', 'медсестра', 'платить', 'внутренне', 'составлять', 'мучать', 'роддом', 'искать', 'наличие', 'успокаивать', 'воронеж', 'выписка', 'порядок', 'плановый', 'роддом', 'начинать', 'ожидать', 'беременность', 'запись', 'узел', 'этаж', 'лечить', 'человек', 'идти', 'договор', 'сервис', 'печение', 'приносить', 'лететь', 'направление', 'запрещать', 'мед', 'раковина', 'момент', 'замериваться', 'ответ', 'заглядывать', 'ярославовна', 'рожать', 'сожаление', 'бокс', 'род', 'область', 'холл', 'счет', 'молодой', 'время', 'общий', 'аппарат', 'короче', 'находиться', 'больница')\n(0.01653988277733369, 0.014545916087558586, 0.014079011091316795, 0.012070381293900244, 0.009433637617105015, 0.008931206681459383, 0.008433586801646816, 0.007874577726422801, 0.007871285786531717, 0.007822749849736863, 0.006973300189790102, 0.0062208407662632656, 0.005756596851611789, 0.005267649517361268, 0.005037218255246565, 0.004669615581285933, 0.004478410611590779, 0.004478365490503721, 0.004463819009128995, 0.00436907397548128, 0.004028356794301686, 0.0035979754047988934, 0.003551982931909629, 0.0035127202162990798, 0.0034026473575794677, 0.0032919649895988834, 0.0032554427331799142, 0.003224964167656175, 0.003216720699115722, 0.003216306660917801, 0.0032029680832764014, 0.003172999016328732, 0.0031659161022000023, 0.003150562342616565, 0.003105331802415036, 0.003081856224097665, 0.003030818715667102, 0.0029995580718613106, 0.002987103087051457, 0.0029339841916230297, 0.0028810485530258363, 0.0028027443746852917, 0.0027749996942482216, 0.002747617927897816, 0.0027238462136163068, 0.002712967800006123, 0.0027033589192158513, 0.00269927763073061, 0.0026950242432936555, 0.00269376988200098, 0.002679219515071252, 0.0026755895016171366, 0.0026679957536078712, 0.0026260307543720086, 0.0026145641470445183, 0.002589007619978145, 0.0025219688931435377, 0.0025163356725615496, 0.0024747343222075507, 0.002456166134818301, 0.0024218256273218454, 0.002419121526298048, 0.002395193517751716, 0.002316437588921524, 0.002309230549733282, 0.0022571158732118696, 0.0022133650875949588, 0.0021955139637297384, 0.002195379555463993, 0.0021758914621136717, 0.002167369133086815, 0.002166796891025855, 0.002160079017041767, 0.002086450267319146, 0.002072343721598526, 0.002066237912492178, 0.0020319408925075166, 0.0020307949741264013, 0.0020010361242016445, 0.0019970098685958186, 0.0019693776193630405, 0.001965792538877904, 0.00194651127397176, 0.0019169973296464652, 0.0018737714948909717, 0.0018632009756913738, 0.0018218385097885609, 0.0017947376655015966, 0.0017902500003935055, 0.001785709985461948, 0.0017799777424210722, 0.001760791244016476, 0.001755702912217595, 0.001743764077883928, 0.0017373996798366956, 0.0017246629276869738, 0.0017071024619661937, 0.0017026015564462747, 0.0017004517563290783, 0.0016964693018171003)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# 4. Вывести наиболее значимые признаки (токены), используя один из указанных методов [3, 4]\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "def feature_importances(X, Y):\n",
    "    model = ExtraTreesClassifier()\n",
    "    model.fit(X, Y)\n",
    "    return model.feature_importances_\n",
    "\n",
    "def tokenize_documents_extend(documents):\n",
    "    texts = []\n",
    "\n",
    "    for document in documents:\n",
    "        w = tokenize(document)\n",
    "        texts.extend(w)\n",
    "\n",
    "    return texts\n",
    "\n",
    "def print_most_valuable_features(tokens, tokens_importances, n=100):\n",
    "    tokens_importances, tokens = zip(*sorted(zip(tokens_importances, tokens), reverse=True))\n",
    "\n",
    "    print(tokens[:n])\n",
    "    print(tokens_importances[:n])\n",
    "\n",
    "log.info('Features importance:')\n",
    "token_importances = feature_importances(X_train_vect.toarray(), y_train)\n",
    "word_arr = tokenize_documents_extend(X_train)\n",
    "print_most_valuable_features(word_arr, token_importances)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "INFO - 2019-05-14 18:34:56,852 - Features char ngram\n",
      "INFO - 2019-05-14 18:34:58,377 - Precision: 0.91, recall: 0.816, f-measure: 0.861\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# 5. Реализовать не менее 5-ти собственных признаков (можно больше), улучшающих результаты классификатора, полученные с использованием признака tf-idf. Оценить поочередно качество классификатора, обученного на tf-idf и каждом из признаков [5].\n",
    "\n",
    "# Features char ngram\n",
    "import numpy as np\n",
    "\n",
    "def get_char_ngram_feature(x_train, x_test):\n",
    "    vectorizer = TfidfVectorizer(max_features=50000, min_df=5, analyzer='char', ngram_range=(3, 3))\n",
    "    X_train_vect_char_ngram = vectorizer.fit_transform(x_train)\n",
    "    X_test_vect_char_ngram = vectorizer.transform(x_test)\n",
    "\n",
    "    return X_train_vect_char_ngram, X_test_vect_char_ngram\n",
    "\n",
    "\n",
    "def get_word_ngram_feature(x_train, x_test):\n",
    "    vectorizer = TfidfVectorizer(max_features=50000, min_df=5, analyzer='word', ngram_range=(3, 3))\n",
    "    X_train_vect_word_ngram = vectorizer.fit_transform(x_train)\n",
    "    X_test_vect_word_ngram = vectorizer.transform(x_test)\n",
    "\n",
    "    return X_train_vect_word_ngram, X_test_vect_word_ngram\n",
    "\n",
    "log.info('Features char ngram')\n",
    "X_train_vect_char_ngram, X_test_vect_char_ngram = get_char_ngram_feature(X_train, X_test)\n",
    "\n",
    "train_matrix = np.append(X_train_vect.toarray(), X_train_vect_char_ngram.toarray(), axis=1)\n",
    "test_matrix = np.append(X_test_vect.toarray(), X_test_vect_char_ngram.toarray(), axis=1)\n",
    "\n",
    "fit_and_print_score(train_matrix, y_train, test_matrix, y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "INFO - 2019-05-14 18:35:05,063 - Features word ngram\n",
      "INFO - 2019-05-14 18:35:07,034 - Precision: 0.896, recall: 0.793, f-measure: 0.841\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# Features word ngram\n",
    "log.info('Features word ngram')\n",
    "X_train_vect_word_ngram, X_test_vect_word_ngram = get_word_ngram_feature(X_train, X_test)\n",
    "\n",
    "train_matrix = np.append(X_train_vect.toarray(), X_train_vect_word_ngram.toarray(), axis=1)\n",
    "test_matrix = np.append(X_test_vect.toarray(), X_test_vect_word_ngram.toarray(), axis=1)\n",
    "\n",
    "fit_and_print_score(train_matrix, y_train, test_matrix, y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "INFO - 2019-05-14 18:36:31,899 - Feature word count\n",
      "INFO - 2019-05-14 18:49:09,872 - Precision: 0.877, recall: 0.816, f-measure: 0.845\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# Feature word count\n",
    "\n",
    "def tokenize_documents_append(documents):\n",
    "    texts = []\n",
    "\n",
    "    for document in documents:\n",
    "        w = tokenize(document)\n",
    "        texts.append(w)\n",
    "\n",
    "    return texts\n",
    "\n",
    "def get_documents_tokens(x_train, x_test):\n",
    "    train_tokens = tokenize_documents_append(x_train)\n",
    "    test_tokens = tokenize_documents_append(x_test)\n",
    "    return train_tokens, test_tokens\n",
    "\n",
    "def get_word_count_feature(train_tokens, test_tokens):\n",
    "    train_word_counts = [len(tokens) for tokens in train_tokens]\n",
    "    train_word_counts = np.reshape(train_word_counts, (-1, 1))\n",
    "\n",
    "    test_word_counts = [len(tokens) for tokens in test_tokens]\n",
    "    test_word_counts = np.reshape(test_word_counts, (-1, 1))\n",
    "\n",
    "    return train_word_counts, test_word_counts\n",
    "\n",
    "log.info('Feature word count')\n",
    "train_tokens, test_tokens = get_documents_tokens(X_train, X_test)\n",
    "\n",
    "train_word_counts, test_word_counts = get_word_count_feature(train_tokens, test_tokens)\n",
    "\n",
    "train_matrix = np.append(X_train_vect.toarray(), train_word_counts, axis=1)\n",
    "test_matrix = np.append(X_test_vect.toarray(), test_word_counts, axis=1)\n",
    "\n",
    "fit_and_print_score(train_matrix, y_train, test_matrix, y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "INFO - 2019-05-14 18:49:10,101 - Feature characters count\n",
      "INFO - 2019-05-14 18:49:10,165 - Precision: 0.888, recall: 0.816, f-measure: 0.845\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# Feature characters count\n",
    "\n",
    "def get_char_count_feature(X_train, X_test):\n",
    "    train_char_counts = [len(document) for document in X_train]\n",
    "    train_char_counts = np.reshape(train_char_counts, (-1, 1))\n",
    "\n",
    "    test_char_counts = [len(document) for document in X_test]\n",
    "    test_char_counts = np.reshape(test_char_counts, (-1, 1))\n",
    "\n",
    "    return train_char_counts, test_char_counts\n",
    "\n",
    "log.info('Feature characters count')\n",
    "train_char_counts, test_char_counts = get_char_count_feature(X_train, X_test)\n",
    "\n",
    "train_matrix = np.append(X_train_vect.toarray(), train_char_counts, axis=1)\n",
    "test_matrix = np.append(X_test_vect.toarray(), test_char_counts, axis=1)\n",
    "\n",
    "fit_and_print_score(train_matrix, y_train, test_matrix, y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "INFO - 2019-05-14 18:49:10,408 - Feature brackets count\n",
      "INFO - 2019-05-14 18:49:10,603 - Precision: 0.855, recall: 0.828, f-measure: 0.841\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# Feature brackets count\n",
    "\n",
    "def get_bracket_count_feature(X_train, X_test):\n",
    "    train_bracket_counts = [document.count(')') - document.count('(') for document in X_train]\n",
    "    train_bracket_counts = np.reshape(train_bracket_counts, (-1, 1))\n",
    "\n",
    "    test_bracket_counts = [document.count(')') - document.count('(') for document in X_test]\n",
    "    test_bracket_counts = np.reshape(test_bracket_counts, (-1, 1))\n",
    "\n",
    "    return train_bracket_counts, test_bracket_counts\n",
    "\n",
    "log.info('Feature brackets count')\n",
    "train_bracket_counts, test_bracket_counts = get_bracket_count_feature(X_train, X_test)\n",
    "\n",
    "train_matrix = np.append(X_train_vect.toarray(), train_bracket_counts, axis=1)\n",
    "test_matrix = np.append(X_test_vect.toarray(), test_bracket_counts, axis=1)\n",
    "\n",
    "fit_and_print_score(train_matrix, y_train, test_matrix, y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "INFO - 2019-05-14 18:49:10,683 - Feature Sentimental\n",
      "INFO - 2019-05-14 18:51:11,055 - Precision: 0.815, recall: 0.839, f-measure: 0.825\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# Feature Sentimental\n",
    "\n",
    "import codecs\n",
    "\n",
    "POSITIVE_PATH = 'data/positive.txt'\n",
    "NEGATIVE_PATH = 'data/negative.txt'\n",
    "\n",
    "\n",
    "def get_sentimental_feature(train_tokens, test_tokens):\n",
    "    positive_words = read_words(POSITIVE_PATH)\n",
    "    negative_words = read_words(NEGATIVE_PATH)\n",
    "\n",
    "    train_sentimental_values = sentimental_values(train_tokens, positive_words, negative_words)\n",
    "    train_sentimental_values = np.reshape(train_sentimental_values, (-1, 1))\n",
    "\n",
    "    test_sentimental_values = sentimental_values(test_tokens, positive_words, negative_words)\n",
    "    test_sentimental_values = np.reshape(test_sentimental_values, (-1, 1))\n",
    "\n",
    "    return train_sentimental_values, test_sentimental_values\n",
    "\n",
    "def read_words(filepath):\n",
    "    with codecs.open(filepath, 'r', 'utf-8') as f:\n",
    "        words = f.readlines()\n",
    "        words = [x.strip() for x in words]\n",
    "        return set(words)\n",
    "\n",
    "def sentimental_values(documents, positive_words, negative_words):\n",
    "    sentimental_values = []\n",
    "\n",
    "    for document in documents:\n",
    "        sentimental_value = 0\n",
    "        for token in document:\n",
    "            if token in positive_words:\n",
    "                sentimental_value += 1\n",
    "            elif token in negative_words:\n",
    "                sentimental_value -= 1\n",
    "\n",
    "        sentimental_values.append(sentimental_value)\n",
    "\n",
    "    return sentimental_values\n",
    "\n",
    "log.info('Feature Sentimental')\n",
    "train_sentimental_values, test_sentimental_values = get_sentimental_feature(train_tokens, test_tokens)\n",
    "\n",
    "train_matrix = np.append(X_train_vect.toarray(), train_sentimental_values, axis=1)\n",
    "test_matrix = np.append(X_test_vect.toarray(), test_sentimental_values, axis=1)\n",
    "\n",
    "fit_and_print_score(train_matrix, y_train, test_matrix, y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "INFO - 2019-05-14 18:51:11,070 - All futures\n",
      "INFO - 2019-05-14 18:52:12,167 - Precision: 0.862, recall: 0.793, f-measure: 0.826\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# 6. Оценить качество классификатора, обученного на tf-idf и всех реализованных признаках.\n",
    "\n",
    "# All features\n",
    "log.info('All futures')\n",
    "train_matrix = np.append(X_train_vect.toarray(), X_train_vect_char_ngram.toarray(), axis=1)\n",
    "test_matrix = np.append(X_test_vect.toarray(), X_test_vect_char_ngram.toarray(), axis=1)\n",
    "\n",
    "train_matrix = np.append(train_matrix, X_train_vect_word_ngram.toarray(), axis=1)\n",
    "test_matrix = np.append(test_matrix, X_test_vect_word_ngram.toarray(), axis=1)\n",
    "\n",
    "train_matrix = np.append(train_matrix, train_word_counts, axis=1)\n",
    "test_matrix = np.append(test_matrix, test_word_counts, axis=1)\n",
    "\n",
    "train_matrix = np.append(train_matrix, train_char_counts, axis=1)\n",
    "test_matrix = np.append(test_matrix, test_char_counts, axis=1)\n",
    "\n",
    "train_matrix = np.append(train_matrix, train_bracket_counts, axis=1)\n",
    "test_matrix = np.append(test_matrix, test_bracket_counts, axis=1)\n",
    "\n",
    "fit_and_print_score(train_matrix, y_train, test_matrix, y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "INFO - 2019-05-14 18:52:12,176 - GridSearchCV features importance:\n",
      "/home/maxim/anaconda3/envs/nlp/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n  warnings.warn(CV_WARNING, FutureWarning)\n/home/maxim/anaconda3/envs/nlp/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/home/maxim/anaconda3/envs/nlp/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n/home/maxim/anaconda3/envs/nlp/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/home/maxim/anaconda3/envs/nlp/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": [
      "('приходиться', 'посмотреть', 'новый', 'рд', 'свой', 'нагрубить', 'наступление', 'рекомендовать', 'никакой', 'деньги', 'бесплатно', 'измерение', 'хирург', 'стоматолог', 'отделение', 'обращаться', 'травма', 'условие', 'род', 'широдхар', 'жалоба', 'навыкат', 'сумма', 'мужик', 'камень', 'ночь', 'плюс', 'минус', 'кстати', 'равный', 'врач', 'специальный', 'поддерживать', 'тихий', 'ждать', 'вечно', 'родственник', 'процедура', 'город', 'приезжать', 'смысл', 'кушать', 'полечить', 'увы', 'линза', 'говорить', 'подарить', 'отдел', 'коммерческий', 'хотя', 'говорить', 'ребенок', 'стоматология', 'прерикаться', 'аккуратно', 'сказать', 'возле', 'чуткий', 'больший', 'е', 'тарпан', 'зуб', 'июнь', 'вылечивать', 'грубый', 'решать', 'санитарка', 'бестолковый', 'переставать', 'никак', 'выход', 'процедура', 'весь', 'рядом', 'гинеколог', 'ответ', 'анализ', 'взвешивание', 'намечаться', 'ирина', 'каждый', 'хирург', 'заслуга', 'год', 'смущать', 'время', 'который', 'сильно', 'роды', 'бывать', 'малоприятный', 'искать', 'порошок', 'окончание', 'палата', 'консультация', 'услуга', 'получать', 'чистота', 'врач')\n(0.008374086529272304, 0.0063751771559133735, 0.004787245979009228, 0.004409460009187685, 0.0043691325141551746, 0.003401412085295532, 0.003328492354439714, 0.00322694595141436, 0.003103554736438548, 0.0029640378125296283, 0.0027320431726118683, 0.002711680457229742, 0.0026966280642617404, 0.002686236429666729, 0.0026592671445635043, 0.0025718618450345687, 0.002554177951958729, 0.0024939914161598996, 0.0024871570020623705, 0.002417088852469357, 0.002276706320956571, 0.0022588547257235345, 0.0022428499279157988, 0.0022193966528497684, 0.002204000167465275, 0.0021687442141739145, 0.0021353496740275837, 0.0021275917551189906, 0.002095788876108643, 0.0020435108827637385, 0.0020397808490992394, 0.0020341320229000513, 0.0019833197280760446, 0.001976029594183943, 0.0019668282346530872, 0.0019640276752779283, 0.0019617139252082224, 0.0019322839645623236, 0.0019145510425397911, 0.0018931185407564995, 0.0018814018753424658, 0.0018750727546069858, 0.0018591860438612745, 0.0018172046924046265, 0.001808692944552304, 0.0017935199969719, 0.0017886425769942516, 0.001759499437109125, 0.0017411679608903882, 0.0017355260171799697, 0.0017278093889329098, 0.0017173492181682634, 0.001712224056747998, 0.0016933201368269355, 0.0016678542552608852, 0.0016675338599561676, 0.0016610293455933598, 0.0016501110724796292, 0.0016492092952266313, 0.001623321763645414, 0.001615393511761705, 0.0016131716350662312, 0.0016097753369202032, 0.0016030486414878465, 0.0015926101721829042, 0.001564495494720874, 0.0015470385129119617, 0.0015461353034479275, 0.0015192082524788865, 0.001517548300124016, 0.001512491228715272, 0.0015085799707536735, 0.001507004270941001, 0.0014874432713698287, 0.0014870031785174773, 0.0014858617760184149, 0.0014834976839027078, 0.0014801369746189432, 0.0014789184173991628, 0.0014789079127261226, 0.0014765431644782325, 0.0014757503713744668, 0.001472571361821474, 0.0014724270688516502, 0.0014542802615939044, 0.001451757449346358, 0.0014426370678463749, 0.0014423846773584886, 0.001441748502340868, 0.0014413721849656403, 0.0014381565513635793, 0.0014296993475725447, 0.0014287946777398256, 0.0014179298848778458, 0.0014168880554253271, 0.001406978429891423, 0.0014001727951090254, 0.0013848052991166002, 0.0013804624944473887, 0.0013676589370286455)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# 7. С помощью GridSearch вывести наиболее значимые признаки [4].\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def feature_importances_gridsearch(X, Y):\n",
    "    model = GridSearchCV(estimator=ExtraTreesClassifier(), param_grid={})\n",
    "    model.fit(X, Y)\n",
    "    return model.best_estimator_.feature_importances_\n",
    "\n",
    "log.info('GridSearchCV features importance:')\n",
    "token_importances = feature_importances_gridsearch(train_matrix, y_train)\n",
    "print_most_valuable_features(word_arr, token_importances)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}