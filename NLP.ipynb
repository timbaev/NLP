{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import logging\n",
    "import string\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from pymystem3 import Mystem\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORPUS_PATH = 'data/corpus.txt'\n",
    "log = logging.getLogger()\n",
    "\n",
    "POS_MARK = 1\n",
    "NEG_MARK = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(corpus_path):\n",
    "    with open(corpus_path, 'r') as f:\n",
    "        log.info(\"Reading file...\")\n",
    "\n",
    "        reviews = []\n",
    "        marks = []\n",
    "\n",
    "        lines = f.readlines()\n",
    "        lines = [x.strip() for x in lines]\n",
    "\n",
    "        for line in lines:\n",
    "            dictionary = ast.literal_eval(line)\n",
    "            reviews.append(dictionary['description'])\n",
    "\n",
    "            recom_author_mark = dictionary['recom_author_mark']\n",
    "            marks.append(parse_label(recom_author_mark))\n",
    "\n",
    "    return reviews, marks\n",
    "\n",
    "\n",
    "def parse_label(recom_author_mark):\n",
    "    if recom_author_mark == 'ДА':\n",
    "        return POS_MARK\n",
    "    elif recom_author_mark == '':\n",
    "        return NEG_MARK\n",
    "    else:\n",
    "        raise Exception(\"Unknown recom_author_mark\")\n",
    "\n",
    "\n",
    "def split_reviews(reviews, marks, n=500):\n",
    "    pos_reviews = []\n",
    "    neg_reviews = []\n",
    "\n",
    "    log.info(f\"Split reviews to {n} positive and {n} negative.\")\n",
    "\n",
    "    for index, review in enumerate(reviews):\n",
    "        if marks[index] == POS_MARK and len(pos_reviews) < n:\n",
    "            pos_reviews.append(review)\n",
    "        elif marks[index] == NEG_MARK and len(neg_reviews) < n:\n",
    "            neg_reviews.append(review)\n",
    "\n",
    "        if len(pos_reviews) == n and len(neg_reviews) == n:\n",
    "            break\n",
    "\n",
    "    return pos_reviews, neg_reviews\n",
    "\n",
    "\n",
    "def tokenize(document):\n",
    "    ignore = set(stopwords.words('russian'))\n",
    "    stem = Mystem()\n",
    "\n",
    "    tokens = stem.lemmatize(document)\n",
    "\n",
    "    tokens = [w.lower() for w in tokens if w not in ignore]\n",
    "    tokens = [w for w in tokens if w not in string.punctuation]\n",
    "    tokens = [w for w in tokens if w.isalpha()]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def tokenize_documents_extend(documents):\n",
    "    texts = []\n",
    "\n",
    "    for document in documents:\n",
    "        w = tokenize(document)\n",
    "        texts.extend(w)\n",
    "\n",
    "    return texts\n",
    "\n",
    "\n",
    "def tokenize_documents_append(documents):\n",
    "    texts = []\n",
    "\n",
    "    for document in documents:\n",
    "        w = tokenize(document)\n",
    "        texts.append(w)\n",
    "\n",
    "    return texts\n",
    "\n",
    "\n",
    "def print_score_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    metrics = precision_recall_fscore_support(y_pred=y_pred, y_true=y_test, average='binary', pos_label=POS_MARK)\n",
    "    log.info(f'Precision: {round(metrics[0], 3)}, recall: {round(metrics[1], 3)}, f-measure: {round(metrics[2], 3)}')\n",
    "\n",
    "\n",
    "def feature_importances(X, Y):\n",
    "    model = ExtraTreesClassifier()\n",
    "    model.fit(X, Y)\n",
    "    return model.feature_importances_\n",
    "\n",
    "\n",
    "def print_most_valuable_features(tokens, tokens_importances, n=100):\n",
    "    tokens_importances, tokens = zip(*sorted(zip(tokens_importances, tokens), reverse=True))\n",
    "\n",
    "    print(tokens[:n])\n",
    "    print(tokens_importances[:n])\n",
    "\n",
    "\n",
    "def setup_logger():\n",
    "    logging.basicConfig(format='%(levelname)s - %(asctime)s - %(message)s')\n",
    "    log.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "def makeFeatureVec(words, model, num_features):\n",
    "    featureVec = np.zeros((num_features,), dtype=\"float32\")\n",
    "\n",
    "    nwords = 0\n",
    "\n",
    "    index2word_set = set(model.wv.index2word)\n",
    "\n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            nwords = nwords + 1\n",
    "            featureVec = np.add(featureVec, model[word])\n",
    "\n",
    "    featureVec = np.divide(featureVec, nwords)\n",
    "    return featureVec\n",
    "\n",
    "\n",
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    counter = 0\n",
    "\n",
    "    reviewFeatureVecs = np.zeros((len(reviews), num_features), dtype=\"float32\")\n",
    "\n",
    "    for review in reviews:\n",
    "        reviewFeatureVecs[counter] = makeFeatureVec(review, model, num_features)\n",
    "        counter = counter + 1\n",
    "\n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 2019-05-02 16:51:46,769 - Reading file...\n",
      "INFO - 2019-05-02 16:52:03,970 - Split reviews to 500 positive and 500 negative.\n"
     ]
    }
   ],
   "source": [
    "reviews, marks = read_data(CORPUS_PATH)\n",
    "pos_reviews, neg_reviews = split_reviews(reviews, marks)\n",
    "\n",
    "y_pos = [1] * len(pos_reviews)\n",
    "y_neg = [0] * len(neg_reviews)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(pos_reviews + neg_reviews, y_pos + y_neg, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext jupyternotify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 2019-05-02 17:13:33,844 - Precision: 0.867, recall: 0.858, f-measure: 0.863\n",
      "/Users/timursafigullin/miniconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%notify\n",
    "vectorizer = TfidfVectorizer(max_features=50000, min_df=5, tokenizer=tokenize)\n",
    "X_train_vect = vectorizer.fit_transform(X_train)\n",
    "X_test_vect = vectorizer.transform(X_test)\n",
    "\n",
    "model = LogisticRegression(solver=\"lbfgs\")\n",
    "model.fit(X_train_vect.toarray(), y_train)\n",
    "print_score_model(model, X_test_vect.toarray(), y_test)\n",
    "\n",
    "token_importances = feature_importances(X_train_vect.toarray(), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 2019-04-30 09:24:12,301 - Precision: 0.867, recall: 0.827, f-measure: 0.847\n"
     ]
    }
   ],
   "source": [
    "%%notify\n",
    "# Test Features word ngram\n",
    "vectorizer = TfidfVectorizer(max_features=50000, min_df=5, tokenizer=tokenize, analyzer='word', ngram_range=(3, 3))\n",
    "X_train_vect_word_ngram = vectorizer.fit_transform(X_train)\n",
    "X_test_vect_word_ngram = vectorizer.transform(X_test)\n",
    "\n",
    "train_matrix = np.append(X_train_vect.toarray(), X_train_vect_word_ngram.toarray(), axis=1)\n",
    "test_matrix = np.append(X_test_vect.toarray(), X_test_vect_word_ngram.toarray(), axis=1)\n",
    "\n",
    "model = LogisticRegression(solver=\"lbfgs\")\n",
    "model.fit(train_matrix, y_train)\n",
    "print_score_model(model, test_matrix, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 2019-04-30 09:25:43,408 - Precision: 0.852, recall: 0.836, f-measure: 0.844\n"
     ]
    }
   ],
   "source": [
    "%%notify\n",
    "# Test Features char ngram\n",
    "vectorizer = TfidfVectorizer(max_features=50000, min_df=5, analyzer='char', ngram_range=(3, 3))\n",
    "X_train_vect_word_ngram = vectorizer.fit_transform(X_train)\n",
    "X_test_vect_word_ngram = vectorizer.transform(X_test)\n",
    "\n",
    "train_matrix = np.append(X_train_vect.toarray(), X_train_vect_word_ngram.toarray(), axis=1)\n",
    "test_matrix = np.append(X_test_vect.toarray(), X_test_vect_word_ngram.toarray(), axis=1)\n",
    "\n",
    "model = LogisticRegression(solver=\"lbfgs\")\n",
    "model.fit(train_matrix, y_train)\n",
    "print_score_model(model, test_matrix, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 2019-05-02 17:33:15,906 - Precision: 0.85, recall: 0.858, f-measure: 0.854\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%notify\n",
    "# Test Feature brackets count\n",
    "train_bracket_counts = [document.count(')') - document.count('(') for document in X_train]\n",
    "train_bracket_counts = np.reshape(train_bracket_counts, (-1, 1))\n",
    "\n",
    "test_bracket_counts = [document.count(')') - document.count('(') for document in X_test]\n",
    "test_bracket_counts = np.reshape(test_bracket_counts, (-1, 1))\n",
    "\n",
    "train_matrix = np.append(X_train_vect.toarray(), train_bracket_counts, axis=1)\n",
    "test_matrix = np.append(X_test_vect.toarray(), test_bracket_counts, axis=1)\n",
    "\n",
    "model = LogisticRegression(solver=\"lbfgs\")\n",
    "model.fit(train_matrix, y_train)\n",
    "print_score_model(model, test_matrix, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 2019-05-02 17:35:51,072 - Precision: 0.835, recall: 0.764, f-measure: 0.798\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%notify\n",
    "# Test Feature characters count\n",
    "train_char_counts = [len(document) for document in X_train]\n",
    "train_char_counts = np.reshape(train_char_counts, (-1, 1))\n",
    "\n",
    "test_char_counts = [len(document) for document in X_test]\n",
    "test_char_counts = np.reshape(test_char_counts, (-1, 1))\n",
    "\n",
    "train_matrix = np.append(X_train_vect.toarray(), train_char_counts, axis=1)\n",
    "test_matrix = np.append(X_test_vect.toarray(), test_char_counts, axis=1)\n",
    "\n",
    "model = LogisticRegression(solver=\"lbfgs\")\n",
    "model.fit(train_matrix, y_train)\n",
    "print_score_model(model, test_matrix, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 2019-05-02 17:57:42,517 - Precision: 0.856, recall: 0.84, f-measure: 0.848\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%notify\n",
    "# Test Feature word count\n",
    "train_sentences = tokenize_documents_append(X_train)\n",
    "train_word_counts = [len(tokens) for tokens in train_sentences]\n",
    "train_word_counts = np.reshape(train_word_counts, (-1, 1))\n",
    "\n",
    "test_sentences = tokenize_documents_append(X_test)\n",
    "test_word_counts = [len(tokens) for tokens in test_sentences]\n",
    "test_word_counts = np.reshape(test_word_counts, (-1, 1))\n",
    "\n",
    "train_matrix = np.append(X_train_vect.toarray(), train_word_counts, axis=1)\n",
    "test_matrix = np.append(X_test_vect.toarray(), test_word_counts, axis=1)\n",
    "\n",
    "model = LogisticRegression(solver=\"lbfgs\")\n",
    "model.fit(train_matrix, y_train)\n",
    "print_score_model(model, test_matrix, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 2019-05-02 18:14:44,146 - collecting all words and their counts\n",
      "INFO - 2019-05-02 18:14:44,148 - PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO - 2019-05-02 18:14:44,193 - collected 12034 word types from a corpus of 116873 raw words and 800 sentences\n",
      "INFO - 2019-05-02 18:14:44,194 - Loading a fresh vocabulary\n",
      "INFO - 2019-05-02 18:14:44,213 - effective_min_count=5 retains 3055 unique words (25% of original 12034, drops 8979)\n",
      "INFO - 2019-05-02 18:14:44,218 - effective_min_count=5 leaves 102677 word corpus (87% of original 116873, drops 14196)\n",
      "INFO - 2019-05-02 18:14:44,232 - deleting the raw counts dictionary of 12034 items\n",
      "INFO - 2019-05-02 18:14:44,234 - sample=0.001 downsamples 46 most-common words\n",
      "INFO - 2019-05-02 18:14:44,235 - downsampling leaves estimated 94094 word corpus (91.6% of prior 102677)\n",
      "INFO - 2019-05-02 18:14:44,255 - estimated required memory for 3055 words and 100 dimensions: 3971500 bytes\n",
      "INFO - 2019-05-02 18:14:44,257 - resetting layer weights\n",
      "INFO - 2019-05-02 18:14:44,313 - training model with 3 workers on 3055 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "INFO - 2019-05-02 18:14:44,439 - worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 2019-05-02 18:14:44,448 - worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 2019-05-02 18:14:44,449 - worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 2019-05-02 18:14:44,450 - EPOCH - 1 : training on 116873 raw words (93963 effective words) took 0.1s, 705841 effective words/s\n",
      "INFO - 2019-05-02 18:14:44,569 - worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 2019-05-02 18:14:44,579 - worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 2019-05-02 18:14:44,580 - worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 2019-05-02 18:14:44,581 - EPOCH - 2 : training on 116873 raw words (94131 effective words) took 0.1s, 747244 effective words/s\n",
      "INFO - 2019-05-02 18:14:44,677 - worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 2019-05-02 18:14:44,687 - worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 2019-05-02 18:14:44,690 - worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 2019-05-02 18:14:44,691 - EPOCH - 3 : training on 116873 raw words (94130 effective words) took 0.1s, 894278 effective words/s\n",
      "INFO - 2019-05-02 18:14:44,817 - worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 2019-05-02 18:14:44,830 - worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 2019-05-02 18:14:44,836 - worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 2019-05-02 18:14:44,837 - EPOCH - 4 : training on 116873 raw words (94057 effective words) took 0.1s, 662150 effective words/s\n",
      "INFO - 2019-05-02 18:14:44,981 - worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 2019-05-02 18:14:44,989 - worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 2019-05-02 18:14:44,993 - worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 2019-05-02 18:14:44,994 - EPOCH - 5 : training on 116873 raw words (93939 effective words) took 0.1s, 654555 effective words/s\n",
      "INFO - 2019-05-02 18:14:44,995 - training on a 584365 raw words (470220 effective words) took 0.7s, 690204 effective words/s\n",
      "/Users/timursafigullin/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:115: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "/Users/timursafigullin/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:117: RuntimeWarning: invalid value encountered in true_divide\n",
      "INFO - 2019-05-02 18:18:45,597 - collecting all words and their counts\n",
      "INFO - 2019-05-02 18:18:45,600 - PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO - 2019-05-02 18:18:45,610 - collected 5150 word types from a corpus of 24785 raw words and 200 sentences\n",
      "INFO - 2019-05-02 18:18:45,610 - Loading a fresh vocabulary\n",
      "INFO - 2019-05-02 18:18:45,614 - effective_min_count=5 retains 1029 unique words (19% of original 5150, drops 4121)\n",
      "INFO - 2019-05-02 18:18:45,615 - effective_min_count=5 leaves 18454 word corpus (74% of original 24785, drops 6331)\n",
      "INFO - 2019-05-02 18:18:45,620 - deleting the raw counts dictionary of 5150 items\n",
      "INFO - 2019-05-02 18:18:45,621 - sample=0.001 downsamples 71 most-common words\n",
      "INFO - 2019-05-02 18:18:45,623 - downsampling leaves estimated 16169 word corpus (87.6% of prior 18454)\n",
      "INFO - 2019-05-02 18:18:45,628 - estimated required memory for 1029 words and 100 dimensions: 1337700 bytes\n",
      "INFO - 2019-05-02 18:18:45,629 - resetting layer weights\n",
      "INFO - 2019-05-02 18:18:45,647 - training model with 3 workers on 1029 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "INFO - 2019-05-02 18:18:45,698 - worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 2019-05-02 18:18:45,712 - worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 2019-05-02 18:18:45,716 - worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 2019-05-02 18:18:45,717 - EPOCH - 1 : training on 24785 raw words (16159 effective words) took 0.1s, 297288 effective words/s\n",
      "INFO - 2019-05-02 18:18:45,746 - worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 2019-05-02 18:18:45,761 - worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 2019-05-02 18:18:45,763 - worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 2019-05-02 18:18:45,766 - EPOCH - 2 : training on 24785 raw words (16183 effective words) took 0.0s, 363008 effective words/s\n",
      "INFO - 2019-05-02 18:18:45,803 - worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 2019-05-02 18:18:45,820 - worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 2019-05-02 18:18:45,835 - worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 2019-05-02 18:18:45,843 - EPOCH - 3 : training on 24785 raw words (16113 effective words) took 0.1s, 260270 effective words/s\n",
      "INFO - 2019-05-02 18:18:45,881 - worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 2019-05-02 18:18:45,883 - worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 2019-05-02 18:18:45,886 - worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 2019-05-02 18:18:45,887 - EPOCH - 4 : training on 24785 raw words (16167 effective words) took 0.0s, 478660 effective words/s\n",
      "INFO - 2019-05-02 18:18:45,925 - worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 2019-05-02 18:18:45,927 - worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 2019-05-02 18:18:45,934 - worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 2019-05-02 18:18:45,935 - EPOCH - 5 : training on 24785 raw words (16156 effective words) took 0.0s, 439205 effective words/s\n",
      "INFO - 2019-05-02 18:18:45,936 - training on a 123925 raw words (80778 effective words) took 0.3s, 285221 effective words/s\n",
      "WARNING - 2019-05-02 18:18:45,938 - under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-0f499626ee7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"lbfgs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_bow_vectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mprint_score_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_bow_vectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m         X, y = check_X_y(X, y, accept_sparse='csr', dtype=_dtype, order=\"C\",\n\u001b[0;32m-> 1288\u001b[0;31m                          accept_large_sparse=solver != 'liblinear')\n\u001b[0m\u001b[1;32m   1289\u001b[0m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1290\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    754\u001b[0m                     \u001b[0mensure_min_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_min_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m                     \u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 756\u001b[0;31m                     estimator=estimator)\n\u001b[0m\u001b[1;32m    757\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m             _assert_all_finite(array,\n\u001b[0;32m--> 573\u001b[0;31m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0m\u001b[1;32m    574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m     \u001b[0mshape_repr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_shape_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan)\u001b[0m\n\u001b[1;32m     54\u001b[0m                 not allow_nan and not np.isfinite(X).all()):\n\u001b[1;32m     55\u001b[0m             \u001b[0mtype_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'infinity'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mallow_nan\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'NaN, infinity'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ],
     "output_type": "error"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%notify\n",
    "# Test Features avg Word2Vec\n",
    "train_sentences = tokenize_documents_append(X_train)\n",
    "word2vec_train_model = Word2Vec(train_sentences)\n",
    "train_avg_vectors = getAvgFeatureVecs(train_sentences, word2vec_train_model, 100)\n",
    "\n",
    "test_sentences = tokenize_documents_append(X_test)\n",
    "word2vec_test_model = Word2Vec(test_sentences)\n",
    "test_avg_vectors = getAvgFeatureVecs(test_sentences, word2vec_test_model, 100)\n",
    "\n",
    "X_train_bow_vectors = np.append(X_train_vect.toarray(), train_avg_vectors, axis=1)\n",
    "X_test_bow_vectors = np.append(X_test_vect.toarray(), test_avg_vectors, axis=1)\n",
    "\n",
    "model = LogisticRegression(tol=1e-16, C=0.1)\n",
    "model.fit(X_train_bow_vectors, y_train)\n",
    "print_score_model(model, X_test_bow_vectors, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
